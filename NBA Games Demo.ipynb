{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdedc16a",
   "metadata": {},
   "source": [
    "# NBA Games Prediction Demo\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e58b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b10",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084dea2",
   "metadata": {},
   "source": [
    "## Basic data exploration\n",
    "This is done as a way to simply have a sense of what the data that you are handling will be like before you dive deeper into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c65449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what the data looks like\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More info about the data\n",
    "train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a52b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the datasets\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9a2ee",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Exploratory Data Analysis (EDA) is a crucial initial phase in data analysis that involves examining and summarizing the main characteristics of a dataset. Its primary objective is to gain insights into the data, understand its underlying structure, identify patterns, and spot anomalies or relationships between variables.\n",
    "\n",
    "### Key Aspects of Exploratory Data Analysis:\n",
    "<u>Data Summarization</u>: EDA involves summarizing the main properties of the dataset using statistical measures like mean, median, standard deviation, etc., to get an overall understanding of the data.\n",
    "\n",
    "<u>Visualization Techniques</u>: Graphical representations such as histograms, box plots, scatter plots, heatmaps, and others are used to visualize distributions, trends, correlations, and outliers in the data. Visualizations help in understanding the data's structure more intuitively.\n",
    "\n",
    "<u>Handling Missing Values</u>: EDA identifies and deals with missing values or inconsistencies within the dataset through imputation or removal to ensure data integrity.\n",
    "\n",
    "<u>Detecting Patterns and Relationships</u>: EDA examines the relationships between variables, seeking correlations, associations, or trends that can provide meaningful insights for further analysis.\n",
    "\n",
    "<u>Outlier Detection</u>: Identification and assessment of outliers or anomalies in the data that might affect statistical analyses or modeling.\n",
    "\n",
    "<u>Feature Engineering Considerations</u>: EDA may involve transforming or engineering features to improve predictive models or reduce dimensionality.\n",
    "\n",
    "<u>Data Preprocessing Insights</u>: Understanding the need for data cleaning, normalization, scaling, or encoding before modeling.\n",
    "\n",
    "### Other Resources for EDA\n",
    "Please refer to the [Datathon Starter Pack]() to see examples on how to conduct EDA and techniques that you can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of points\n",
    "sns.histplot(data=train, x='pts', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f107917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of fg%\n",
    "sns.histplot(data=train, x='fg%', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of 3p%\n",
    "sns.histplot(data=train, x='3p%', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a45021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the number of wins and losses of the Toronto Raptors\n",
    "sns.countplot(data=train[train['team']=='TOR'], x='won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the correlation between points and the result of the game\n",
    "sns.boxplot(data=train, x='won', y='pts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854f502",
   "metadata": {},
   "source": [
    "## Data Cleaning/Data Pre-processing/Feature Engineering\n",
    "\n",
    "Data scientists engage in data cleaning and feature engineering as essential steps before model creation for several crucial reasons:\n",
    "\n",
    "1. <u>Ensuring Data Quality</u>:\n",
    "Data cleaning involves handling missing values, correcting errors, and addressing inconsistencies. This process ensures the dataset's integrity and reliability, reducing biases or inaccuracies that could mislead model predictions.\n",
    "2. <u>Enhancing Model Performance</u>:\n",
    "Feature engineering involves transforming raw data into meaningful and predictive features. Creating new features or modifying existing ones helps the model better capture patterns and relationships within the data, leading to improved predictive performance.\n",
    "3. <u>Mitigating Model Overfitting</u>:\n",
    "Data cleaning and feature engineering help in reducing noise and irrelevant information in the dataset. By selecting or engineering relevant features, the model becomes less prone to overfitting, making it more generalizable to new data.\n",
    "4. <u>Handling Dimensionality</u>:\n",
    "Feature engineering can reduce dimensionality by selecting or creating a subset of the most informative features. This simplifies the model, making it computationally efficient while retaining essential information for accurate predictions.\n",
    "5. <u>Adapting to Model Assumptions</u>:\n",
    "Data cleaning ensures that the data meets assumptions required by certain models. For instance, linear models assume no multicollinearity, and feature engineering can address this by handling correlated features.\n",
    "6. <u>Improving Interpretability</u>:\n",
    "Well-engineered features can enhance interpretability of the model's predictions. Creating features that align with domain knowledge allows stakeholders to comprehend and trust the model's outputs.\n",
    "7. <u>Preparing Data for Various Models</u>:\n",
    "Different models have diverse requirements. Data cleaning and feature engineering enable preparing the data to fit specific model algorithms, maximizing their effectiveness.\n",
    "\n",
    "In summary, data cleaning and feature engineering lay the groundwork for accurate, robust, and reliable predictive models. These steps not only refine the dataset but also empower models to effectively learn patterns and relationships, enhancing their performance and applicability in solving real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date and resetting the index\n",
    "train = train.sort_values(\"date\")\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.sort_values(\"date\")\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# Delete unnecessary columns in both train and test datasets (these are the \"max\" columns)\n",
    "train = train.drop(train.iloc[:, 34:66],axis = 1)\n",
    "train = train.drop(train.iloc[:, 71:103],axis = 1)\n",
    "train = train.drop([\"index_opp\",\"mp_opp\"],axis=1)\n",
    "\n",
    "test = test.drop(test.iloc[:, 34:66],axis = 1)\n",
    "test = test.drop(test.iloc[:, 71:103],axis = 1)\n",
    "test = test.drop([\"index_opp\",\"mp_opp\"],axis=1)\n",
    "\n",
    "# Changing the 'won' column from True/False to 1's and 0's\n",
    "train['won'] = train['won']*1\n",
    "test['won'] = test['won']*1\n",
    "\n",
    "# Separating the date column into 3 columns: year, month, day\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "\n",
    "train['day'] = train['date'].dt.day\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "train = train.drop(\"date\",axis=1)\n",
    "\n",
    "test['day'] = test['date'].dt.day\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "test = test.drop(\"date\",axis=1)\n",
    "\n",
    "# Creating dummy variables for categorical variables\n",
    "train = pd.get_dummies(train, columns=['team', 'team_opp'], drop_first=True)\n",
    "test = pd.get_dummies(test, columns=['team', 'team_opp'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad461f",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "The model training phase is a core step in machine learning where a selected algorithm learns patterns and relationships within the provided data. This process involves:\n",
    "\n",
    "<u>Data Input</u>: Using a portion of the prepared dataset known as the training set.\n",
    "\n",
    "<u>Algorithm Application</u>: Employing the chosen machine learning algorithm to analyze the training data. The algorithm adjusts its internal parameters to recognize patterns that link input features to the desired output or target.\n",
    "\n",
    "<u>Optimization</u>: Through repeated iterations, the algorithm fine-tunes its parameters to minimize the difference between predicted outcomes and actual results.\n",
    "\n",
    "<u>Evaluation</u>: Assessing the model's performance using evaluation metrics specific to the problem type (accuracy, loss, etc.) to ensure its effectiveness in capturing patterns and making accurate predictions.\n",
    "\n",
    "<u>Iterative Process</u>: Often requiring multiple iterations to adjust the model settings or parameters (such as learning rate or complexity) for improved performance.\n",
    "\n",
    "The goal of model training is to enable the algorithm to learn from the data, allowing it to generalize well on new, unseen data, and make accurate predictions or classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66409049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into X and y\n",
    "X = train.drop(\"target\",axis=1)\n",
    "y = train['target']\n",
    "\n",
    "# Splitting the train data into another train and test set\n",
    "# This is done to verify that our model is not overfitting\n",
    "# NOTE: This test set is different from our original test set because \n",
    "#       this test is set is derived from our original train set. \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580af4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the metrics\n",
    "print(\"The balanced accuracy score of the model is: \" + str(balanced_accuracy_score(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5d35",
   "metadata": {},
   "source": [
    "# Predictions for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model for the official test set\n",
    "final_model = LogisticRegression()\n",
    "final_model.fit(X,y)\n",
    "final_predictions = final_model.predict(test)\n",
    "\n",
    "# Creating a new dataframe in the required submission format\n",
    "submission = pd.DataFrame(test['game_id'])\n",
    "submission['target'] = final_predictions\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a29aa",
   "metadata": {},
   "source": [
    "# Saving csv file for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
